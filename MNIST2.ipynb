{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from numpy.linalg import svd\n",
    "from scipy.linalg import subspace_angles\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy import linalg as LA\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1\n",
    "C=0.2\n",
    "# reduced dimensions\n",
    "START_EPOCH = 0\n",
    "END_EPOCH = 40\n",
    "n_components = 1\n",
    "NUM_EPOCHS_FINE_TUNE = 40\n",
    "save_dir = f'MNIST_labelnoise{C}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_param_vec(model):\n",
    "    \"\"\"\n",
    "    Return model parameters as a vector\n",
    "    \"\"\"\n",
    "    vec = []\n",
    "    for name,param in model.named_parameters():\n",
    "        vec.append(param.detach().cpu().numpy().reshape(-1))\n",
    "    return np.concatenate(vec, 0)\n",
    "\n",
    "def get_model_grad_vec(model):\n",
    "    # Return the model grad as a vector\n",
    "\n",
    "    vec = []\n",
    "    for name,param in model.named_parameters():\n",
    "        vec.append(param.grad.detach().reshape(-1))\n",
    "    return torch.cat(vec, 0)\n",
    "\n",
    "def update_grad(model, grad_vec):\n",
    "    idx = 0\n",
    "    for name,param in model.named_parameters():\n",
    "        arr_shape = param.grad.shape\n",
    "        size = 1\n",
    "        for i in range(len(list(arr_shape))):\n",
    "            size *= arr_shape[i]\n",
    "        param.grad.data = grad_vec[idx:idx+size].reshape(arr_shape)\n",
    "        idx += size\n",
    "\n",
    "def update_param(model, param_vec):\n",
    "    idx = 0\n",
    "    for name,param in model.named_parameters():\n",
    "        arr_shape = param.data.shape\n",
    "        size = 1\n",
    "        for i in range(len(list(arr_shape))):\n",
    "            size *= arr_shape[i]\n",
    "        param.data = param_vec[idx:idx+size].reshape(arr_shape)\n",
    "        idx += size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_grad_vec(model):\n",
    "    \"\"\"Return the gradient of the model as a flattened vector.\"\"\"\n",
    "    vec = []\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            vec.append(param.grad.detach().reshape(-1))\n",
    "    return torch.cat(vec, 0)\n",
    "\n",
    "def update_grad(model, grad_vec):\n",
    "    \"\"\"Update the model gradients with a new flattened gradient vector.\"\"\"\n",
    "    idx = 0\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            arr_shape = param.grad.shape\n",
    "            size = param.grad.numel()\n",
    "            param.grad.data = grad_vec[idx:idx + size].reshape(arr_shape)\n",
    "            idx += size\n",
    "def load_saved_parameters(save_dir, start_epoch, end_epoch):\n",
    "    W = []\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        param_filename = os.path.join(save_dir, f'{epoch + 1}.pt')\n",
    "        if os.path.exists(param_filename):\n",
    "            model.load_state_dict(torch.load(param_filename))\n",
    "            W.append(get_model_param_vec(model))\n",
    "        else:\n",
    "            print(f'File not found: {param_filename}')\n",
    "    W = np.array(W)\n",
    "    print(f'Loaded {len(W)} parameter vectors with shape: {W.shape}')\n",
    "    return W\n",
    "\n",
    "\n",
    "def get_model_param_vec(model):\n",
    "    vec = []\n",
    "    for name, param in model.named_parameters():\n",
    "        vec.append(param.detach().cpu().numpy().reshape(-1))\n",
    "    return np.concatenate(vec, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#PSGD\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m W \u001b[38;5;241m=\u001b[39m \u001b[43mload_saved_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTART_EPOCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEND_EPOCH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m   \u001b[38;5;66;03m# Obtain base variables through PCA\u001b[39;00m\n\u001b[1;32m      9\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39mn_components)\n",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m, in \u001b[0;36mload_saved_parameters\u001b[0;34m(save_dir, start_epoch, end_epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m param_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(param_filename):\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(param_filename))\n\u001b[1;32m     24\u001b[0m     W\u001b[38;5;241m.\u001b[39mappend(get_model_param_vec(model))\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#PSGD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "W = load_saved_parameters(save_dir, START_EPOCH, END_EPOCH)\n",
    "  # Obtain base variables through PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit_transform(W)\n",
    "P = np.array(pca.components_)\n",
    "print ('ratio:', pca.explained_variance_ratio_)\n",
    "print ('P:', P.shape)\n",
    "print(P.dtype)\n",
    "\n",
    "P = torch.from_numpy(P).cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "# Training parameters\n",
    "\n",
    "alpha = 0.1  # Learning rate for residual gradient\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_FINE_TUNE):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, probas = model(features)\n",
    "        cost = criterion(logits, targets)\n",
    "\n",
    "        # Backward pass to compute gradients\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "\n",
    "        # Get the full gradient as a vector\n",
    "        grad_vec = []\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_vec.append(param.grad.detach().reshape(-1))\n",
    "        grad_vec = torch.cat(grad_vec, 0)\n",
    "\n",
    "        # Project gradient to the reduced space\n",
    "        gk = torch.mm(P, grad_vec.reshape(-1, 1))\n",
    "        grad_proj = torch.mm(P.T, gk).reshape(-1)\n",
    "\n",
    "        # Compute residual gradient\n",
    "        grad_res = grad_vec - grad_proj\n",
    "\n",
    "        # Update the model parameters using projected gradient\n",
    "        idx = 0\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                size = param.grad.numel()\n",
    "                param.grad.data = grad_proj[idx:idx + size].reshape(param.grad.shape)\n",
    "                idx += size\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update model with residual gradient using a smaller learning rate (alpha)\n",
    "        idx = 0\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                size = param.grad.numel()\n",
    "                param.grad.data = alpha * grad_res[idx:idx + size].reshape(param.grad.shape)\n",
    "                idx += size\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging every 50 batches\n",
    "        #if batch_idx % 50 == 0:\n",
    "        #    print(f'Epoch: {epoch + 1}/{NUM_EPOCHS_FINE_TUNE} | Batch {batch_idx}/{len(train_loader)} | Cost: {cost:.4f}')\n",
    "\n",
    "    # Evaluate the model after each epoch\n",
    "    model.eval()\n",
    "    train_acc = compute_accuracy(model, train_loader, DEVICE)\n",
    "    test_acc = compute_accuracy(model, test_loader, DEVICE)\n",
    "    print(f'Epoch {epoch + 1}/{NUM_EPOCHS_FINE_TUNE} | Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}%')\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TME\n",
    "\n",
    "\n",
    "def m_estimator(X):\n",
    "    N, D = X.shape\n",
    "    initcov = np.eye(D)  \n",
    "    oldcov = initcov - 1\n",
    "    cov = initcov\n",
    "    iter_count = 1\n",
    "    eps = 1e-10  \n",
    "\n",
    "    while np.linalg.norm(oldcov - cov, 'fro') > 1e-12 and iter_count < 1000:\n",
    "        temp = X @ np.linalg.inv(cov + eps * np.eye(D))  \n",
    "        d = np.sum(temp * np.conjugate(X), axis=1)  \n",
    "        oldcov = cov\n",
    "\n",
    "       \n",
    "        temp = (np.real(d) + eps * np.ones(N))**(-1)  \n",
    "\n",
    "        \n",
    "        temp_matrix = np.diag(temp)  \n",
    "        cov = (X.T @ temp_matrix @ X) / (N * D)  \n",
    "        cov = cov / np.trace(cov)  \n",
    "        iter_count += 1  \n",
    "\n",
    "    return cov\n",
    "\n",
    "\n",
    "\n",
    "W = load_saved_parameters(save_dir, START_EPOCH, END_EPOCH)\n",
    "  # Obtain base variables through PCA\n",
    "V = np.dot(W,W.T)\n",
    "W_hat = sqrtm(V)\n",
    "   \n",
    "Cov = m_estimator(W_hat)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit_transform(Cov)\n",
    "U = np.array(pca.components_)\n",
    "print('U:',U.shape)\n",
    "P =  (W.T) @ LA.inv(W_hat) @ (U.T)\n",
    "    \n",
    "print ('P:', P.shape)\n",
    "P = P.T\n",
    "P = P.astype(np.float32)\n",
    "\n",
    "P = torch.from_numpy(P).cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "# Training parameters\n",
    "\n",
    "alpha = 0.1  # Learning rate for residual gradient\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_FINE_TUNE):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, probas = model(features)\n",
    "        cost = criterion(logits, targets)\n",
    "\n",
    "        # Backward pass to compute gradients\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "\n",
    "        # Get the full gradient as a vector\n",
    "        grad_vec = []\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_vec.append(param.grad.detach().reshape(-1))\n",
    "        grad_vec = torch.cat(grad_vec, 0)\n",
    "\n",
    "        # Project gradient to the reduced space\n",
    "        gk = torch.mm(P, grad_vec.reshape(-1, 1))\n",
    "        grad_proj = torch.mm(P.T, gk).reshape(-1)\n",
    "\n",
    "        # Compute residual gradient\n",
    "        grad_res = grad_vec - grad_proj\n",
    "\n",
    "        # Update the model parameters using projected gradient\n",
    "        idx = 0\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                size = param.grad.numel()\n",
    "                param.grad.data = grad_proj[idx:idx + size].reshape(param.grad.shape)\n",
    "                idx += size\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update model with residual gradient using a smaller learning rate (alpha)\n",
    "        idx = 0\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                size = param.grad.numel()\n",
    "                param.grad.data = alpha * grad_res[idx:idx + size].reshape(param.grad.shape)\n",
    "                idx += size\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging every 50 batches\n",
    "        #if batch_idx % 50 == 0:\n",
    "        #    print(f'Epoch: {epoch + 1}/{NUM_EPOCHS_FINE_TUNE} | Batch {batch_idx}/{len(train_loader)} | Cost: {cost:.4f}')\n",
    "\n",
    "    # Evaluate the model after each epoch\n",
    "    model.eval()\n",
    "    train_acc = compute_accuracy(model, train_loader, DEVICE)\n",
    "    test_acc = compute_accuracy(model, test_loader, DEVICE)\n",
    "    print(f'Epoch {epoch + 1}/{NUM_EPOCHS_FINE_TUNE} | Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}%')\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FMS\n",
    "\n",
    "def FMS(X, dd):\n",
    "    D, N = X.shape\n",
    "\n",
    "    # Initial iteration count\n",
    "    iter = 1\n",
    "\n",
    "    # Perform SVD and initialize L\n",
    "    U, _, _ = svd(X, full_matrices=False)\n",
    "    L = U[:, :dd]\n",
    "\n",
    "    # Set initial angle and tolerance\n",
    "    ang = 1\n",
    "\n",
    "    # Iterate until convergence or max iteration count\n",
    "    while ang > 1e-12 and iter < 1000:\n",
    "        Lold = L\n",
    "\n",
    "        # Compute the residual projection\n",
    "        temp = (np.eye(D) - L @ L.T) @ X\n",
    "        w = np.sqrt(np.sum(temp**2, axis=0)) + 1e-10\n",
    "\n",
    "        # Reweight and update XX\n",
    "        XX = X @ np.diag(1.0 / w) @ X.T\n",
    "\n",
    "        # Perform SVD again on the weighted matrix XX\n",
    "        U, _, _ = svd(XX, full_matrices=False)\n",
    "        L = U[:, :dd]\n",
    "\n",
    "        # Compute the angle between new and old subspace\n",
    "        ang = np.linalg.norm(subspace_angles(L, Lold))\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "    return L\n",
    "\n",
    "W = load_saved_parameters(save_dir, START_EPOCH, END_EPOCH)\n",
    "  # Obtain base variables through PCA\n",
    "V = np.dot(W,W.T)\n",
    "W_hat = sqrtm(V)\n",
    "n_components = n_components\n",
    "U= FMS(W_hat,n_components)\n",
    "    \n",
    "P =  (W.T) @ LA.inv(W_hat) @ (U)\n",
    "    \n",
    "print ('P:', P.shape)\n",
    "P = P.T\n",
    "P = P.astype(np.float32)\n",
    "P = torch.from_numpy(P).cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "# Training parameters\n",
    "\n",
    "alpha = 0.1  # Learning rate for residual gradient\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_FINE_TUNE):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, probas = model(features)\n",
    "        cost = criterion(logits, targets)\n",
    "\n",
    "        # Backward pass to compute gradients\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "\n",
    "        # Get the full gradient as a vector\n",
    "        grad_vec = []\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_vec.append(param.grad.detach().reshape(-1))\n",
    "        grad_vec = torch.cat(grad_vec, 0)\n",
    "\n",
    "        # Project gradient to the reduced space\n",
    "        gk = torch.mm(P, grad_vec.reshape(-1, 1))\n",
    "        grad_proj = torch.mm(P.T, gk).reshape(-1)\n",
    "\n",
    "        # Compute residual gradient\n",
    "        grad_res = grad_vec - grad_proj\n",
    "\n",
    "        # Update the model parameters using projected gradient\n",
    "        idx = 0\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                size = param.grad.numel()\n",
    "                param.grad.data = grad_proj[idx:idx + size].reshape(param.grad.shape)\n",
    "                idx += size\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update model with residual gradient using a smaller learning rate (alpha)\n",
    "        idx = 0\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                size = param.grad.numel()\n",
    "                param.grad.data = alpha * grad_res[idx:idx + size].reshape(param.grad.shape)\n",
    "                idx += size\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging every 50 batches\n",
    "        #if batch_idx % 50 == 0:\n",
    "        #    print(f'Epoch: {epoch + 1}/{NUM_EPOCHS_FINE_TUNE} | Batch {batch_idx}/{len(train_loader)} | Cost: {cost:.4f}')\n",
    "\n",
    "    # Evaluate the model after each epoch\n",
    "    model.eval()\n",
    "    train_acc = compute_accuracy(model, train_loader, DEVICE)\n",
    "    test_acc = compute_accuracy(model, test_loader, DEVICE)\n",
    "    print(f'Epoch {epoch + 1}/{NUM_EPOCHS_FINE_TUNE} | Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}%')\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
