{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from numpy.linalg import svd\n",
    "from scipy.linalg import subspace_angles\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy import linalg as LA\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Hyperparameters\n",
    "RANDOM_SEED = 1\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "WORKERS = 2\n",
    "NUM_EPOCHS = 40\n",
    "C = 0 #corruption level\n",
    "save_dir = f'MNIST_labelnoise{C}'\n",
    "\n",
    "# Architecture\n",
    "NUM_FEATURES = 28*28\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Other\n",
    "DEVICE = \"cuda:0\"\n",
    "GRAYSCALE = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: MNIST_labelnoise0\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    print(f'Created directory: {save_dir}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_mnist_loaders(batch_size=64, workers=4, corrupt=0.0, seed=42):\n",
    "    \"\"\"\n",
    "    Prepare DataLoader for MNIST dataset with optional label corruption.\n",
    "    \n",
    "    Parameters:\n",
    "        batch_size (int): Batch size for training and testing.\n",
    "        workers (int): Number of data loading workers.\n",
    "        corrupt (float): Corruption level (0 to 1).\n",
    "        seed (int): Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        train_loader, test_loader: DataLoaders for MNIST.\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Normalize MNIST dataset with mean and std values\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    # Load the MNIST training dataset\n",
    "    train_dataset = datasets.MNIST(\n",
    "        root='data',\n",
    "        train=True,\n",
    "        transform=transform,\n",
    "        download=True\n",
    "    )\n",
    "\n",
    "    # Corrupt labels if corruption level is specified\n",
    "    if corrupt > 0:\n",
    "        print(f'Applying {corrupt*100}% label corruption...')\n",
    "        num_samples = len(train_dataset.targets)\n",
    "        num_corrupt = int(num_samples * corrupt)\n",
    "        corrupt_indices = np.random.choice(num_samples, num_corrupt, replace=False)\n",
    "        \n",
    "        for idx in corrupt_indices:\n",
    "            original_label = train_dataset.targets[idx].item()\n",
    "            new_label = np.random.choice([x for x in range(10) if x != original_label])\n",
    "            train_dataset.targets[idx] = new_label\n",
    "\n",
    "    # Load the MNIST test dataset (without corruption)\n",
    "    test_dataset = datasets.MNIST(\n",
    "        root='data',\n",
    "        train=False,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=workers\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=workers\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loader, test_loader = get_mnist_loaders(batch_size=BATCH_SIZE, workers=WORKERS, corrupt=C)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Batch index: 0 | Batch size: 128\n",
      "Epoch: 2 | Batch index: 0 | Batch size: 128\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(DEVICE)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "for epoch in range(2):\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        \n",
    "        print('Epoch:', epoch+1, end='')\n",
    "        print(' | Batch index:', batch_idx, end='')\n",
    "        print(' | Batch size:', y.size()[0])\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes, grayscale):\n",
    "        self.inplanes = 64\n",
    "        if grayscale:\n",
    "            in_dim = 1\n",
    "        else:\n",
    "            in_dim = 3\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_dim, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, (2. / n)**.5)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        # because MNIST is already 1x1 here:\n",
    "        # disable avg pooling\n",
    "        #x = self.avgpool(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc(x)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas\n",
    "\n",
    "\n",
    "\n",
    "def resnet18(num_classes):\n",
    "    \"\"\"Constructs a ResNet-18 model.\"\"\"\n",
    "    model = ResNet(block=BasicBlock, \n",
    "                   layers=[2, 2, 2, 2],\n",
    "                   num_classes=NUM_CLASSES,\n",
    "                   grayscale=GRAYSCALE)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/040 | Batch 0000/0469 | Cost: 2.6219\n",
      "Epoch: 001/040 | Batch 0050/0469 | Cost: 0.1830\n",
      "Epoch: 001/040 | Batch 0100/0469 | Cost: 0.1559\n",
      "Epoch: 001/040 | Batch 0150/0469 | Cost: 0.0656\n",
      "Epoch: 001/040 | Batch 0200/0469 | Cost: 0.1591\n",
      "Epoch: 001/040 | Batch 0250/0469 | Cost: 0.1879\n",
      "Epoch: 001/040 | Batch 0300/0469 | Cost: 0.0329\n",
      "Epoch: 001/040 | Batch 0350/0469 | Cost: 0.1030\n",
      "Epoch: 001/040 | Batch 0400/0469 | Cost: 0.1210\n",
      "Epoch: 001/040 | Batch 0450/0469 | Cost: 0.1472\n",
      "Epoch: 001/040 | Train: 98.172%\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 002/040 | Batch 0000/0469 | Cost: 0.0902\n",
      "Epoch: 002/040 | Batch 0050/0469 | Cost: 0.0462\n",
      "Epoch: 002/040 | Batch 0100/0469 | Cost: 0.0333\n",
      "Epoch: 002/040 | Batch 0150/0469 | Cost: 0.0369\n",
      "Epoch: 002/040 | Batch 0200/0469 | Cost: 0.0319\n",
      "Epoch: 002/040 | Batch 0250/0469 | Cost: 0.0355\n",
      "Epoch: 002/040 | Batch 0300/0469 | Cost: 0.0269\n",
      "Epoch: 002/040 | Batch 0350/0469 | Cost: 0.0469\n",
      "Epoch: 002/040 | Batch 0400/0469 | Cost: 0.0170\n",
      "Epoch: 002/040 | Batch 0450/0469 | Cost: 0.0639\n",
      "Epoch: 002/040 | Train: 98.638%\n",
      "Time elapsed: 0.43 min\n",
      "Epoch: 003/040 | Batch 0000/0469 | Cost: 0.0263\n",
      "Epoch: 003/040 | Batch 0050/0469 | Cost: 0.0696\n",
      "Epoch: 003/040 | Batch 0100/0469 | Cost: 0.0284\n",
      "Epoch: 003/040 | Batch 0150/0469 | Cost: 0.0209\n",
      "Epoch: 003/040 | Batch 0200/0469 | Cost: 0.1154\n",
      "Epoch: 003/040 | Batch 0250/0469 | Cost: 0.0540\n",
      "Epoch: 003/040 | Batch 0300/0469 | Cost: 0.0319\n",
      "Epoch: 003/040 | Batch 0350/0469 | Cost: 0.0186\n",
      "Epoch: 003/040 | Batch 0400/0469 | Cost: 0.0243\n",
      "Epoch: 003/040 | Batch 0450/0469 | Cost: 0.0205\n",
      "Epoch: 003/040 | Train: 99.182%\n",
      "Time elapsed: 0.57 min\n",
      "Epoch: 004/040 | Batch 0000/0469 | Cost: 0.0513\n",
      "Epoch: 004/040 | Batch 0050/0469 | Cost: 0.0040\n",
      "Epoch: 004/040 | Batch 0100/0469 | Cost: 0.0273\n",
      "Epoch: 004/040 | Batch 0150/0469 | Cost: 0.0372\n",
      "Epoch: 004/040 | Batch 0200/0469 | Cost: 0.0067\n",
      "Epoch: 004/040 | Batch 0250/0469 | Cost: 0.0742\n",
      "Epoch: 004/040 | Batch 0300/0469 | Cost: 0.2100\n",
      "Epoch: 004/040 | Batch 0350/0469 | Cost: 0.0028\n",
      "Epoch: 004/040 | Batch 0400/0469 | Cost: 0.0449\n",
      "Epoch: 004/040 | Batch 0450/0469 | Cost: 0.0035\n",
      "Epoch: 004/040 | Train: 98.918%\n",
      "Time elapsed: 0.70 min\n",
      "Epoch: 005/040 | Batch 0000/0469 | Cost: 0.0413\n",
      "Epoch: 005/040 | Batch 0050/0469 | Cost: 0.0089\n",
      "Epoch: 005/040 | Batch 0100/0469 | Cost: 0.1111\n",
      "Epoch: 005/040 | Batch 0150/0469 | Cost: 0.0370\n",
      "Epoch: 005/040 | Batch 0200/0469 | Cost: 0.0052\n",
      "Epoch: 005/040 | Batch 0250/0469 | Cost: 0.0571\n",
      "Epoch: 005/040 | Batch 0300/0469 | Cost: 0.0294\n",
      "Epoch: 005/040 | Batch 0350/0469 | Cost: 0.0385\n",
      "Epoch: 005/040 | Batch 0400/0469 | Cost: 0.0266\n",
      "Epoch: 005/040 | Batch 0450/0469 | Cost: 0.0331\n",
      "Epoch: 005/040 | Train: 99.143%\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 006/040 | Batch 0000/0469 | Cost: 0.0257\n",
      "Epoch: 006/040 | Batch 0050/0469 | Cost: 0.0501\n",
      "Epoch: 006/040 | Batch 0100/0469 | Cost: 0.0008\n",
      "Epoch: 006/040 | Batch 0150/0469 | Cost: 0.0845\n",
      "Epoch: 006/040 | Batch 0200/0469 | Cost: 0.0106\n",
      "Epoch: 006/040 | Batch 0250/0469 | Cost: 0.0109\n",
      "Epoch: 006/040 | Batch 0300/0469 | Cost: 0.0022\n",
      "Epoch: 006/040 | Batch 0350/0469 | Cost: 0.0074\n",
      "Epoch: 006/040 | Batch 0400/0469 | Cost: 0.0432\n",
      "Epoch: 006/040 | Batch 0450/0469 | Cost: 0.0478\n",
      "Epoch: 006/040 | Train: 99.150%\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 007/040 | Batch 0000/0469 | Cost: 0.0153\n",
      "Epoch: 007/040 | Batch 0050/0469 | Cost: 0.0013\n",
      "Epoch: 007/040 | Batch 0100/0469 | Cost: 0.0253\n",
      "Epoch: 007/040 | Batch 0150/0469 | Cost: 0.0346\n",
      "Epoch: 007/040 | Batch 0200/0469 | Cost: 0.0111\n",
      "Epoch: 007/040 | Batch 0250/0469 | Cost: 0.0141\n",
      "Epoch: 007/040 | Batch 0300/0469 | Cost: 0.0187\n",
      "Epoch: 007/040 | Batch 0350/0469 | Cost: 0.0287\n",
      "Epoch: 007/040 | Batch 0400/0469 | Cost: 0.0155\n",
      "Epoch: 007/040 | Batch 0450/0469 | Cost: 0.0316\n",
      "Epoch: 007/040 | Train: 99.662%\n",
      "Time elapsed: 1.11 min\n",
      "Epoch: 008/040 | Batch 0000/0469 | Cost: 0.0039\n",
      "Epoch: 008/040 | Batch 0050/0469 | Cost: 0.0189\n",
      "Epoch: 008/040 | Batch 0100/0469 | Cost: 0.0437\n",
      "Epoch: 008/040 | Batch 0150/0469 | Cost: 0.0006\n",
      "Epoch: 008/040 | Batch 0200/0469 | Cost: 0.0117\n",
      "Epoch: 008/040 | Batch 0250/0469 | Cost: 0.0353\n",
      "Epoch: 008/040 | Batch 0300/0469 | Cost: 0.1018\n",
      "Epoch: 008/040 | Batch 0350/0469 | Cost: 0.0056\n",
      "Epoch: 008/040 | Batch 0400/0469 | Cost: 0.0225\n",
      "Epoch: 008/040 | Batch 0450/0469 | Cost: 0.0905\n",
      "Epoch: 008/040 | Train: 99.535%\n",
      "Time elapsed: 1.24 min\n",
      "Epoch: 009/040 | Batch 0000/0469 | Cost: 0.0191\n",
      "Epoch: 009/040 | Batch 0050/0469 | Cost: 0.0097\n",
      "Epoch: 009/040 | Batch 0100/0469 | Cost: 0.0007\n",
      "Epoch: 009/040 | Batch 0150/0469 | Cost: 0.0642\n",
      "Epoch: 009/040 | Batch 0200/0469 | Cost: 0.0380\n",
      "Epoch: 009/040 | Batch 0250/0469 | Cost: 0.0100\n",
      "Epoch: 009/040 | Batch 0300/0469 | Cost: 0.0634\n",
      "Epoch: 009/040 | Batch 0350/0469 | Cost: 0.0004\n",
      "Epoch: 009/040 | Batch 0400/0469 | Cost: 0.0048\n",
      "Epoch: 009/040 | Batch 0450/0469 | Cost: 0.0075\n",
      "Epoch: 009/040 | Train: 99.618%\n",
      "Time elapsed: 1.38 min\n",
      "Epoch: 010/040 | Batch 0000/0469 | Cost: 0.0050\n",
      "Epoch: 010/040 | Batch 0050/0469 | Cost: 0.0071\n",
      "Epoch: 010/040 | Batch 0100/0469 | Cost: 0.0020\n",
      "Epoch: 010/040 | Batch 0150/0469 | Cost: 0.0077\n",
      "Epoch: 010/040 | Batch 0200/0469 | Cost: 0.0021\n",
      "Epoch: 010/040 | Batch 0250/0469 | Cost: 0.0002\n",
      "Epoch: 010/040 | Batch 0300/0469 | Cost: 0.0916\n",
      "Epoch: 010/040 | Batch 0350/0469 | Cost: 0.0463\n",
      "Epoch: 010/040 | Batch 0400/0469 | Cost: 0.0109\n",
      "Epoch: 010/040 | Batch 0450/0469 | Cost: 0.0144\n",
      "Epoch: 010/040 | Train: 99.503%\n",
      "Time elapsed: 1.51 min\n",
      "Epoch: 011/040 | Batch 0000/0469 | Cost: 0.0060\n",
      "Epoch: 011/040 | Batch 0050/0469 | Cost: 0.0006\n",
      "Epoch: 011/040 | Batch 0100/0469 | Cost: 0.0173\n",
      "Epoch: 011/040 | Batch 0150/0469 | Cost: 0.0512\n",
      "Epoch: 011/040 | Batch 0200/0469 | Cost: 0.0079\n",
      "Epoch: 011/040 | Batch 0250/0469 | Cost: 0.0135\n",
      "Epoch: 011/040 | Batch 0300/0469 | Cost: 0.0011\n",
      "Epoch: 011/040 | Batch 0350/0469 | Cost: 0.0089\n",
      "Epoch: 011/040 | Batch 0400/0469 | Cost: 0.0241\n",
      "Epoch: 011/040 | Batch 0450/0469 | Cost: 0.0106\n",
      "Epoch: 011/040 | Train: 99.453%\n",
      "Time elapsed: 1.65 min\n",
      "Epoch: 012/040 | Batch 0000/0469 | Cost: 0.0163\n",
      "Epoch: 012/040 | Batch 0050/0469 | Cost: 0.0058\n",
      "Epoch: 012/040 | Batch 0100/0469 | Cost: 0.0396\n",
      "Epoch: 012/040 | Batch 0150/0469 | Cost: 0.0207\n",
      "Epoch: 012/040 | Batch 0200/0469 | Cost: 0.0006\n",
      "Epoch: 012/040 | Batch 0250/0469 | Cost: 0.0006\n",
      "Epoch: 012/040 | Batch 0300/0469 | Cost: 0.0464\n",
      "Epoch: 012/040 | Batch 0350/0469 | Cost: 0.0897\n",
      "Epoch: 012/040 | Batch 0400/0469 | Cost: 0.0725\n",
      "Epoch: 012/040 | Batch 0450/0469 | Cost: 0.0031\n",
      "Epoch: 012/040 | Train: 99.442%\n",
      "Time elapsed: 1.78 min\n",
      "Epoch: 013/040 | Batch 0000/0469 | Cost: 0.0089\n",
      "Epoch: 013/040 | Batch 0050/0469 | Cost: 0.0078\n",
      "Epoch: 013/040 | Batch 0100/0469 | Cost: 0.0003\n",
      "Epoch: 013/040 | Batch 0150/0469 | Cost: 0.0125\n",
      "Epoch: 013/040 | Batch 0200/0469 | Cost: 0.0099\n",
      "Epoch: 013/040 | Batch 0250/0469 | Cost: 0.0347\n",
      "Epoch: 013/040 | Batch 0300/0469 | Cost: 0.0449\n",
      "Epoch: 013/040 | Batch 0350/0469 | Cost: 0.0118\n",
      "Epoch: 013/040 | Batch 0400/0469 | Cost: 0.0020\n",
      "Epoch: 013/040 | Batch 0450/0469 | Cost: 0.0167\n",
      "Epoch: 013/040 | Train: 99.703%\n",
      "Time elapsed: 1.91 min\n",
      "Epoch: 014/040 | Batch 0000/0469 | Cost: 0.0013\n",
      "Epoch: 014/040 | Batch 0050/0469 | Cost: 0.0168\n",
      "Epoch: 014/040 | Batch 0100/0469 | Cost: 0.0003\n",
      "Epoch: 014/040 | Batch 0150/0469 | Cost: 0.0014\n",
      "Epoch: 014/040 | Batch 0200/0469 | Cost: 0.0172\n",
      "Epoch: 014/040 | Batch 0250/0469 | Cost: 0.0130\n",
      "Epoch: 014/040 | Batch 0300/0469 | Cost: 0.0017\n",
      "Epoch: 014/040 | Batch 0350/0469 | Cost: 0.0071\n",
      "Epoch: 014/040 | Batch 0400/0469 | Cost: 0.0139\n",
      "Epoch: 014/040 | Batch 0450/0469 | Cost: 0.0143\n",
      "Epoch: 014/040 | Train: 99.713%\n",
      "Time elapsed: 2.05 min\n",
      "Epoch: 015/040 | Batch 0000/0469 | Cost: 0.0067\n",
      "Epoch: 015/040 | Batch 0050/0469 | Cost: 0.0031\n",
      "Epoch: 015/040 | Batch 0100/0469 | Cost: 0.0012\n",
      "Epoch: 015/040 | Batch 0150/0469 | Cost: 0.0554\n",
      "Epoch: 015/040 | Batch 0200/0469 | Cost: 0.0003\n",
      "Epoch: 015/040 | Batch 0250/0469 | Cost: 0.0075\n",
      "Epoch: 015/040 | Batch 0300/0469 | Cost: 0.0012\n",
      "Epoch: 015/040 | Batch 0350/0469 | Cost: 0.0006\n",
      "Epoch: 015/040 | Batch 0400/0469 | Cost: 0.0133\n",
      "Epoch: 015/040 | Batch 0450/0469 | Cost: 0.0003\n",
      "Epoch: 015/040 | Train: 99.398%\n",
      "Time elapsed: 2.19 min\n",
      "Epoch: 016/040 | Batch 0000/0469 | Cost: 0.0015\n",
      "Epoch: 016/040 | Batch 0050/0469 | Cost: 0.0213\n",
      "Epoch: 016/040 | Batch 0100/0469 | Cost: 0.0096\n",
      "Epoch: 016/040 | Batch 0150/0469 | Cost: 0.0327\n",
      "Epoch: 016/040 | Batch 0200/0469 | Cost: 0.0004\n",
      "Epoch: 016/040 | Batch 0250/0469 | Cost: 0.0335\n",
      "Epoch: 016/040 | Batch 0300/0469 | Cost: 0.0096\n",
      "Epoch: 016/040 | Batch 0350/0469 | Cost: 0.0219\n",
      "Epoch: 016/040 | Batch 0400/0469 | Cost: 0.0001\n",
      "Epoch: 016/040 | Batch 0450/0469 | Cost: 0.0139\n",
      "Epoch: 016/040 | Train: 99.675%\n",
      "Time elapsed: 2.32 min\n",
      "Epoch: 017/040 | Batch 0000/0469 | Cost: 0.0085\n",
      "Epoch: 017/040 | Batch 0050/0469 | Cost: 0.0002\n",
      "Epoch: 017/040 | Batch 0100/0469 | Cost: 0.0010\n",
      "Epoch: 017/040 | Batch 0150/0469 | Cost: 0.0039\n",
      "Epoch: 017/040 | Batch 0200/0469 | Cost: 0.0028\n",
      "Epoch: 017/040 | Batch 0250/0469 | Cost: 0.0238\n",
      "Epoch: 017/040 | Batch 0300/0469 | Cost: 0.0203\n",
      "Epoch: 017/040 | Batch 0350/0469 | Cost: 0.0045\n",
      "Epoch: 017/040 | Batch 0400/0469 | Cost: 0.0024\n",
      "Epoch: 017/040 | Batch 0450/0469 | Cost: 0.0607\n",
      "Epoch: 017/040 | Train: 99.815%\n",
      "Time elapsed: 2.45 min\n",
      "Epoch: 018/040 | Batch 0000/0469 | Cost: 0.0100\n",
      "Epoch: 018/040 | Batch 0050/0469 | Cost: 0.0040\n",
      "Epoch: 018/040 | Batch 0100/0469 | Cost: 0.0204\n",
      "Epoch: 018/040 | Batch 0150/0469 | Cost: 0.0060\n",
      "Epoch: 018/040 | Batch 0200/0469 | Cost: 0.0004\n",
      "Epoch: 018/040 | Batch 0250/0469 | Cost: 0.0022\n",
      "Epoch: 018/040 | Batch 0300/0469 | Cost: 0.0047\n",
      "Epoch: 018/040 | Batch 0350/0469 | Cost: 0.0139\n",
      "Epoch: 018/040 | Batch 0400/0469 | Cost: 0.0126\n",
      "Epoch: 018/040 | Batch 0450/0469 | Cost: 0.0004\n",
      "Epoch: 018/040 | Train: 99.852%\n",
      "Time elapsed: 2.59 min\n",
      "Epoch: 019/040 | Batch 0000/0469 | Cost: 0.0002\n",
      "Epoch: 019/040 | Batch 0050/0469 | Cost: 0.0160\n",
      "Epoch: 019/040 | Batch 0100/0469 | Cost: 0.0035\n",
      "Epoch: 019/040 | Batch 0150/0469 | Cost: 0.0001\n",
      "Epoch: 019/040 | Batch 0200/0469 | Cost: 0.0001\n",
      "Epoch: 019/040 | Batch 0250/0469 | Cost: 0.0009\n",
      "Epoch: 019/040 | Batch 0300/0469 | Cost: 0.0061\n",
      "Epoch: 019/040 | Batch 0350/0469 | Cost: 0.0013\n",
      "Epoch: 019/040 | Batch 0400/0469 | Cost: 0.0092\n",
      "Epoch: 019/040 | Batch 0450/0469 | Cost: 0.0072\n",
      "Epoch: 019/040 | Train: 99.553%\n",
      "Time elapsed: 2.73 min\n",
      "Epoch: 020/040 | Batch 0000/0469 | Cost: 0.0229\n",
      "Epoch: 020/040 | Batch 0050/0469 | Cost: 0.0031\n",
      "Epoch: 020/040 | Batch 0100/0469 | Cost: 0.0006\n",
      "Epoch: 020/040 | Batch 0150/0469 | Cost: 0.0024\n",
      "Epoch: 020/040 | Batch 0200/0469 | Cost: 0.0354\n",
      "Epoch: 020/040 | Batch 0250/0469 | Cost: 0.0030\n",
      "Epoch: 020/040 | Batch 0300/0469 | Cost: 0.0065\n",
      "Epoch: 020/040 | Batch 0350/0469 | Cost: 0.0021\n",
      "Epoch: 020/040 | Batch 0400/0469 | Cost: 0.0007\n",
      "Epoch: 020/040 | Batch 0450/0469 | Cost: 0.0030\n",
      "Epoch: 020/040 | Train: 99.882%\n",
      "Time elapsed: 2.86 min\n",
      "Epoch: 021/040 | Batch 0000/0469 | Cost: 0.0213\n",
      "Epoch: 021/040 | Batch 0050/0469 | Cost: 0.0263\n",
      "Epoch: 021/040 | Batch 0100/0469 | Cost: 0.0016\n",
      "Epoch: 021/040 | Batch 0150/0469 | Cost: 0.0004\n",
      "Epoch: 021/040 | Batch 0200/0469 | Cost: 0.0021\n",
      "Epoch: 021/040 | Batch 0250/0469 | Cost: 0.0808\n",
      "Epoch: 021/040 | Batch 0300/0469 | Cost: 0.0362\n",
      "Epoch: 021/040 | Batch 0350/0469 | Cost: 0.0012\n",
      "Epoch: 021/040 | Batch 0400/0469 | Cost: 0.0006\n",
      "Epoch: 021/040 | Batch 0450/0469 | Cost: 0.0016\n",
      "Epoch: 021/040 | Train: 99.857%\n",
      "Time elapsed: 3.00 min\n",
      "Epoch: 022/040 | Batch 0000/0469 | Cost: 0.0003\n",
      "Epoch: 022/040 | Batch 0050/0469 | Cost: 0.0015\n",
      "Epoch: 022/040 | Batch 0100/0469 | Cost: 0.0095\n",
      "Epoch: 022/040 | Batch 0150/0469 | Cost: 0.0016\n",
      "Epoch: 022/040 | Batch 0200/0469 | Cost: 0.0000\n",
      "Epoch: 022/040 | Batch 0250/0469 | Cost: 0.0001\n",
      "Epoch: 022/040 | Batch 0300/0469 | Cost: 0.0001\n",
      "Epoch: 022/040 | Batch 0350/0469 | Cost: 0.0034\n",
      "Epoch: 022/040 | Batch 0400/0469 | Cost: 0.0022\n",
      "Epoch: 022/040 | Batch 0450/0469 | Cost: 0.0179\n",
      "Epoch: 022/040 | Train: 99.802%\n",
      "Time elapsed: 3.13 min\n",
      "Epoch: 023/040 | Batch 0000/0469 | Cost: 0.0010\n",
      "Epoch: 023/040 | Batch 0050/0469 | Cost: 0.0004\n",
      "Epoch: 023/040 | Batch 0100/0469 | Cost: 0.0388\n",
      "Epoch: 023/040 | Batch 0150/0469 | Cost: 0.0005\n",
      "Epoch: 023/040 | Batch 0200/0469 | Cost: 0.0001\n",
      "Epoch: 023/040 | Batch 0250/0469 | Cost: 0.0017\n",
      "Epoch: 023/040 | Batch 0300/0469 | Cost: 0.0023\n",
      "Epoch: 023/040 | Batch 0350/0469 | Cost: 0.0005\n",
      "Epoch: 023/040 | Batch 0400/0469 | Cost: 0.0000\n",
      "Epoch: 023/040 | Batch 0450/0469 | Cost: 0.0189\n",
      "Epoch: 023/040 | Train: 99.732%\n",
      "Time elapsed: 3.27 min\n",
      "Epoch: 024/040 | Batch 0000/0469 | Cost: 0.0006\n",
      "Epoch: 024/040 | Batch 0050/0469 | Cost: 0.0061\n",
      "Epoch: 024/040 | Batch 0100/0469 | Cost: 0.0001\n",
      "Epoch: 024/040 | Batch 0150/0469 | Cost: 0.0342\n",
      "Epoch: 024/040 | Batch 0200/0469 | Cost: 0.0398\n",
      "Epoch: 024/040 | Batch 0250/0469 | Cost: 0.0011\n",
      "Epoch: 024/040 | Batch 0300/0469 | Cost: 0.0110\n",
      "Epoch: 024/040 | Batch 0350/0469 | Cost: 0.0013\n",
      "Epoch: 024/040 | Batch 0400/0469 | Cost: 0.0710\n",
      "Epoch: 024/040 | Batch 0450/0469 | Cost: 0.0521\n",
      "Epoch: 024/040 | Train: 99.907%\n",
      "Time elapsed: 3.41 min\n",
      "Epoch: 025/040 | Batch 0000/0469 | Cost: 0.0001\n",
      "Epoch: 025/040 | Batch 0050/0469 | Cost: 0.0024\n",
      "Epoch: 025/040 | Batch 0100/0469 | Cost: 0.0001\n",
      "Epoch: 025/040 | Batch 0150/0469 | Cost: 0.0019\n",
      "Epoch: 025/040 | Batch 0200/0469 | Cost: 0.0017\n",
      "Epoch: 025/040 | Batch 0250/0469 | Cost: 0.0009\n",
      "Epoch: 025/040 | Batch 0300/0469 | Cost: 0.0030\n",
      "Epoch: 025/040 | Batch 0350/0469 | Cost: 0.0004\n",
      "Epoch: 025/040 | Batch 0400/0469 | Cost: 0.0000\n",
      "Epoch: 025/040 | Batch 0450/0469 | Cost: 0.0071\n",
      "Epoch: 025/040 | Train: 99.820%\n",
      "Time elapsed: 3.54 min\n",
      "Epoch: 026/040 | Batch 0000/0469 | Cost: 0.0002\n",
      "Epoch: 026/040 | Batch 0050/0469 | Cost: 0.1569\n",
      "Epoch: 026/040 | Batch 0100/0469 | Cost: 0.0004\n",
      "Epoch: 026/040 | Batch 0150/0469 | Cost: 0.0001\n",
      "Epoch: 026/040 | Batch 0200/0469 | Cost: 0.0042\n",
      "Epoch: 026/040 | Batch 0250/0469 | Cost: 0.0000\n",
      "Epoch: 026/040 | Batch 0300/0469 | Cost: 0.0000\n",
      "Epoch: 026/040 | Batch 0350/0469 | Cost: 0.0168\n",
      "Epoch: 026/040 | Batch 0400/0469 | Cost: 0.0343\n",
      "Epoch: 026/040 | Batch 0450/0469 | Cost: 0.0005\n",
      "Epoch: 026/040 | Train: 99.968%\n",
      "Time elapsed: 3.68 min\n",
      "Epoch: 027/040 | Batch 0000/0469 | Cost: 0.0001\n",
      "Epoch: 027/040 | Batch 0050/0469 | Cost: 0.0035\n",
      "Epoch: 027/040 | Batch 0100/0469 | Cost: 0.0000\n",
      "Epoch: 027/040 | Batch 0150/0469 | Cost: 0.0001\n",
      "Epoch: 027/040 | Batch 0200/0469 | Cost: 0.0156\n",
      "Epoch: 027/040 | Batch 0250/0469 | Cost: 0.0016\n",
      "Epoch: 027/040 | Batch 0300/0469 | Cost: 0.0052\n",
      "Epoch: 027/040 | Batch 0350/0469 | Cost: 0.0046\n",
      "Epoch: 027/040 | Batch 0400/0469 | Cost: 0.0024\n",
      "Epoch: 027/040 | Batch 0450/0469 | Cost: 0.0047\n",
      "Epoch: 027/040 | Train: 99.925%\n",
      "Time elapsed: 3.81 min\n",
      "Epoch: 028/040 | Batch 0000/0469 | Cost: 0.0003\n",
      "Epoch: 028/040 | Batch 0050/0469 | Cost: 0.0156\n",
      "Epoch: 028/040 | Batch 0100/0469 | Cost: 0.0006\n",
      "Epoch: 028/040 | Batch 0150/0469 | Cost: 0.0050\n",
      "Epoch: 028/040 | Batch 0200/0469 | Cost: 0.0027\n",
      "Epoch: 028/040 | Batch 0250/0469 | Cost: 0.0020\n",
      "Epoch: 028/040 | Batch 0300/0469 | Cost: 0.0074\n",
      "Epoch: 028/040 | Batch 0350/0469 | Cost: 0.0004\n",
      "Epoch: 028/040 | Batch 0400/0469 | Cost: 0.0003\n",
      "Epoch: 028/040 | Batch 0450/0469 | Cost: 0.0023\n",
      "Epoch: 028/040 | Train: 99.880%\n",
      "Time elapsed: 3.95 min\n",
      "Epoch: 029/040 | Batch 0000/0469 | Cost: 0.0004\n",
      "Epoch: 029/040 | Batch 0050/0469 | Cost: 0.0001\n",
      "Epoch: 029/040 | Batch 0100/0469 | Cost: 0.0003\n",
      "Epoch: 029/040 | Batch 0150/0469 | Cost: 0.0016\n",
      "Epoch: 029/040 | Batch 0200/0469 | Cost: 0.0001\n",
      "Epoch: 029/040 | Batch 0250/0469 | Cost: 0.0005\n",
      "Epoch: 029/040 | Batch 0300/0469 | Cost: 0.0001\n",
      "Epoch: 029/040 | Batch 0350/0469 | Cost: 0.0264\n",
      "Epoch: 029/040 | Batch 0400/0469 | Cost: 0.0015\n",
      "Epoch: 029/040 | Batch 0450/0469 | Cost: 0.0000\n",
      "Epoch: 029/040 | Train: 99.875%\n",
      "Time elapsed: 4.09 min\n",
      "Epoch: 030/040 | Batch 0000/0469 | Cost: 0.0010\n",
      "Epoch: 030/040 | Batch 0050/0469 | Cost: 0.0015\n",
      "Epoch: 030/040 | Batch 0100/0469 | Cost: 0.0191\n",
      "Epoch: 030/040 | Batch 0150/0469 | Cost: 0.0227\n",
      "Epoch: 030/040 | Batch 0200/0469 | Cost: 0.0014\n",
      "Epoch: 030/040 | Batch 0250/0469 | Cost: 0.0005\n",
      "Epoch: 030/040 | Batch 0300/0469 | Cost: 0.0000\n",
      "Epoch: 030/040 | Batch 0350/0469 | Cost: 0.0019\n",
      "Epoch: 030/040 | Batch 0400/0469 | Cost: 0.0005\n",
      "Epoch: 030/040 | Batch 0450/0469 | Cost: 0.0011\n",
      "Epoch: 030/040 | Train: 99.963%\n",
      "Time elapsed: 4.22 min\n",
      "Epoch: 031/040 | Batch 0000/0469 | Cost: 0.0002\n",
      "Epoch: 031/040 | Batch 0050/0469 | Cost: 0.0000\n",
      "Epoch: 031/040 | Batch 0100/0469 | Cost: 0.0016\n",
      "Epoch: 031/040 | Batch 0150/0469 | Cost: 0.0001\n",
      "Epoch: 031/040 | Batch 0200/0469 | Cost: 0.0001\n",
      "Epoch: 031/040 | Batch 0250/0469 | Cost: 0.0006\n",
      "Epoch: 031/040 | Batch 0300/0469 | Cost: 0.0008\n",
      "Epoch: 031/040 | Batch 0350/0469 | Cost: 0.0002\n",
      "Epoch: 031/040 | Batch 0400/0469 | Cost: 0.0011\n",
      "Epoch: 031/040 | Batch 0450/0469 | Cost: 0.0003\n",
      "Epoch: 031/040 | Train: 99.925%\n",
      "Time elapsed: 4.36 min\n",
      "Epoch: 032/040 | Batch 0000/0469 | Cost: 0.0000\n",
      "Epoch: 032/040 | Batch 0050/0469 | Cost: 0.0001\n",
      "Epoch: 032/040 | Batch 0100/0469 | Cost: 0.0040\n",
      "Epoch: 032/040 | Batch 0150/0469 | Cost: 0.0001\n",
      "Epoch: 032/040 | Batch 0200/0469 | Cost: 0.0010\n",
      "Epoch: 032/040 | Batch 0250/0469 | Cost: 0.0020\n",
      "Epoch: 032/040 | Batch 0300/0469 | Cost: 0.0004\n",
      "Epoch: 032/040 | Batch 0350/0469 | Cost: 0.0005\n",
      "Epoch: 032/040 | Batch 0400/0469 | Cost: 0.0005\n",
      "Epoch: 032/040 | Batch 0450/0469 | Cost: 0.0038\n",
      "Epoch: 032/040 | Train: 99.895%\n",
      "Time elapsed: 4.49 min\n",
      "Epoch: 033/040 | Batch 0000/0469 | Cost: 0.0002\n",
      "Epoch: 033/040 | Batch 0050/0469 | Cost: 0.0106\n",
      "Epoch: 033/040 | Batch 0100/0469 | Cost: 0.0000\n",
      "Epoch: 033/040 | Batch 0150/0469 | Cost: 0.0011\n",
      "Epoch: 033/040 | Batch 0200/0469 | Cost: 0.0022\n",
      "Epoch: 033/040 | Batch 0250/0469 | Cost: 0.0003\n",
      "Epoch: 033/040 | Batch 0300/0469 | Cost: 0.0001\n",
      "Epoch: 033/040 | Batch 0350/0469 | Cost: 0.0021\n",
      "Epoch: 033/040 | Batch 0400/0469 | Cost: 0.0013\n",
      "Epoch: 033/040 | Batch 0450/0469 | Cost: 0.0101\n",
      "Epoch: 033/040 | Train: 99.933%\n",
      "Time elapsed: 4.63 min\n",
      "Epoch: 034/040 | Batch 0000/0469 | Cost: 0.0000\n",
      "Epoch: 034/040 | Batch 0050/0469 | Cost: 0.0000\n",
      "Epoch: 034/040 | Batch 0100/0469 | Cost: 0.0012\n",
      "Epoch: 034/040 | Batch 0150/0469 | Cost: 0.0001\n",
      "Epoch: 034/040 | Batch 0200/0469 | Cost: 0.0001\n",
      "Epoch: 034/040 | Batch 0250/0469 | Cost: 0.0000\n",
      "Epoch: 034/040 | Batch 0300/0469 | Cost: 0.0054\n",
      "Epoch: 034/040 | Batch 0350/0469 | Cost: 0.0013\n",
      "Epoch: 034/040 | Batch 0400/0469 | Cost: 0.0001\n",
      "Epoch: 034/040 | Batch 0450/0469 | Cost: 0.0058\n",
      "Epoch: 034/040 | Train: 99.752%\n",
      "Time elapsed: 4.76 min\n",
      "Epoch: 035/040 | Batch 0000/0469 | Cost: 0.0002\n",
      "Epoch: 035/040 | Batch 0050/0469 | Cost: 0.0253\n",
      "Epoch: 035/040 | Batch 0100/0469 | Cost: 0.0002\n",
      "Epoch: 035/040 | Batch 0150/0469 | Cost: 0.0032\n",
      "Epoch: 035/040 | Batch 0200/0469 | Cost: 0.0034\n",
      "Epoch: 035/040 | Batch 0250/0469 | Cost: 0.0000\n",
      "Epoch: 035/040 | Batch 0300/0469 | Cost: 0.0201\n",
      "Epoch: 035/040 | Batch 0350/0469 | Cost: 0.0013\n",
      "Epoch: 035/040 | Batch 0400/0469 | Cost: 0.0001\n",
      "Epoch: 035/040 | Batch 0450/0469 | Cost: 0.0013\n",
      "Epoch: 035/040 | Train: 99.952%\n",
      "Time elapsed: 4.90 min\n",
      "Epoch: 036/040 | Batch 0000/0469 | Cost: 0.0005\n",
      "Epoch: 036/040 | Batch 0050/0469 | Cost: 0.0002\n",
      "Epoch: 036/040 | Batch 0100/0469 | Cost: 0.0008\n",
      "Epoch: 036/040 | Batch 0150/0469 | Cost: 0.0000\n",
      "Epoch: 036/040 | Batch 0200/0469 | Cost: 0.0006\n",
      "Epoch: 036/040 | Batch 0250/0469 | Cost: 0.0000\n",
      "Epoch: 036/040 | Batch 0300/0469 | Cost: 0.0001\n",
      "Epoch: 036/040 | Batch 0350/0469 | Cost: 0.0001\n",
      "Epoch: 036/040 | Batch 0400/0469 | Cost: 0.0000\n",
      "Epoch: 036/040 | Batch 0450/0469 | Cost: 0.0184\n",
      "Epoch: 036/040 | Train: 99.930%\n",
      "Time elapsed: 5.04 min\n",
      "Epoch: 037/040 | Batch 0000/0469 | Cost: 0.0005\n",
      "Epoch: 037/040 | Batch 0050/0469 | Cost: 0.0000\n",
      "Epoch: 037/040 | Batch 0100/0469 | Cost: 0.0029\n",
      "Epoch: 037/040 | Batch 0150/0469 | Cost: 0.0001\n",
      "Epoch: 037/040 | Batch 0200/0469 | Cost: 0.0062\n",
      "Epoch: 037/040 | Batch 0250/0469 | Cost: 0.0000\n",
      "Epoch: 037/040 | Batch 0300/0469 | Cost: 0.0000\n",
      "Epoch: 037/040 | Batch 0350/0469 | Cost: 0.0185\n",
      "Epoch: 037/040 | Batch 0400/0469 | Cost: 0.0000\n",
      "Epoch: 037/040 | Batch 0450/0469 | Cost: 0.0013\n",
      "Epoch: 037/040 | Train: 99.900%\n",
      "Time elapsed: 5.17 min\n",
      "Epoch: 038/040 | Batch 0000/0469 | Cost: 0.0004\n",
      "Epoch: 038/040 | Batch 0050/0469 | Cost: 0.0000\n",
      "Epoch: 038/040 | Batch 0100/0469 | Cost: 0.0160\n",
      "Epoch: 038/040 | Batch 0150/0469 | Cost: 0.0003\n",
      "Epoch: 038/040 | Batch 0200/0469 | Cost: 0.0002\n",
      "Epoch: 038/040 | Batch 0250/0469 | Cost: 0.0004\n",
      "Epoch: 038/040 | Batch 0300/0469 | Cost: 0.0063\n",
      "Epoch: 038/040 | Batch 0350/0469 | Cost: 0.0003\n",
      "Epoch: 038/040 | Batch 0400/0469 | Cost: 0.0017\n",
      "Epoch: 038/040 | Batch 0450/0469 | Cost: 0.0006\n",
      "Epoch: 038/040 | Train: 99.938%\n",
      "Time elapsed: 5.31 min\n",
      "Epoch: 039/040 | Batch 0000/0469 | Cost: 0.0011\n",
      "Epoch: 039/040 | Batch 0050/0469 | Cost: 0.0000\n",
      "Epoch: 039/040 | Batch 0100/0469 | Cost: 0.0002\n",
      "Epoch: 039/040 | Batch 0150/0469 | Cost: 0.0004\n",
      "Epoch: 039/040 | Batch 0200/0469 | Cost: 0.0007\n",
      "Epoch: 039/040 | Batch 0250/0469 | Cost: 0.0001\n",
      "Epoch: 039/040 | Batch 0300/0469 | Cost: 0.0001\n",
      "Epoch: 039/040 | Batch 0350/0469 | Cost: 0.0003\n",
      "Epoch: 039/040 | Batch 0400/0469 | Cost: 0.0000\n",
      "Epoch: 039/040 | Batch 0450/0469 | Cost: 0.0000\n",
      "Epoch: 039/040 | Train: 99.992%\n",
      "Time elapsed: 5.44 min\n",
      "Epoch: 040/040 | Batch 0000/0469 | Cost: 0.0000\n",
      "Epoch: 040/040 | Batch 0050/0469 | Cost: 0.0001\n",
      "Epoch: 040/040 | Batch 0100/0469 | Cost: 0.0001\n",
      "Epoch: 040/040 | Batch 0150/0469 | Cost: 0.0025\n",
      "Epoch: 040/040 | Batch 0200/0469 | Cost: 0.0000\n",
      "Epoch: 040/040 | Batch 0250/0469 | Cost: 0.0000\n",
      "Epoch: 040/040 | Batch 0300/0469 | Cost: 0.0000\n",
      "Epoch: 040/040 | Batch 0350/0469 | Cost: 0.0000\n",
      "Epoch: 040/040 | Batch 0400/0469 | Cost: 0.0000\n",
      "Epoch: 040/040 | Batch 0450/0469 | Cost: 0.0001\n",
      "Epoch: 040/040 | Train: 99.978%\n",
      "Time elapsed: 5.58 min\n",
      "Total Training Time: 5.58 min\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "model = resnet18(NUM_CLASSES)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  \n",
    "\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if not batch_idx % 50:\n",
    "            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
    "                   %(epoch+1, NUM_EPOCHS, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "\n",
    "        \n",
    "\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False): # save memory during inference\n",
    "        print('Epoch: %03d/%03d | Train: %.3f%%' % (\n",
    "              epoch+1, NUM_EPOCHS, \n",
    "              compute_accuracy(model, train_loader, device=DEVICE)))\n",
    "        \n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "    param_filename = os.path.join(save_dir, str(epoch+1 ) + '.pt')\n",
    "    torch.save(model.state_dict(), param_filename)\n",
    "    \n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 99.42%\n"
     ]
    }
   ],
   "source": [
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader, device=DEVICE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_param_vec(model):\n",
    "    \"\"\"\n",
    "    Return model parameters as a vector\n",
    "    \"\"\"\n",
    "    vec = []\n",
    "    for name,param in model.named_parameters():\n",
    "        vec.append(param.detach().cpu().numpy().reshape(-1))\n",
    "    return np.concatenate(vec, 0)\n",
    "\n",
    "def get_model_grad_vec(model):\n",
    "    # Return the model grad as a vector\n",
    "\n",
    "    vec = []\n",
    "    for name,param in model.named_parameters():\n",
    "        vec.append(param.grad.detach().reshape(-1))\n",
    "    return torch.cat(vec, 0)\n",
    "\n",
    "def update_grad(model, grad_vec):\n",
    "    idx = 0\n",
    "    for name,param in model.named_parameters():\n",
    "        arr_shape = param.grad.shape\n",
    "        size = 1\n",
    "        for i in range(len(list(arr_shape))):\n",
    "            size *= arr_shape[i]\n",
    "        param.grad.data = grad_vec[idx:idx+size].reshape(arr_shape)\n",
    "        idx += size\n",
    "\n",
    "def update_param(model, param_vec):\n",
    "    idx = 0\n",
    "    for name,param in model.named_parameters():\n",
    "        arr_shape = param.data.shape\n",
    "        size = 1\n",
    "        for i in range(len(list(arr_shape))):\n",
    "            size *= arr_shape[i]\n",
    "        param.data = param_vec[idx:idx+size].reshape(arr_shape)\n",
    "        idx += size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced dimensions\n",
    "START_EPOCH = 0\n",
    "END_EPOCH = 40\n",
    "n_components = 20\n",
    "NUM_EPOCHS_FINE_TUNE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_grad_vec(model):\n",
    "    \"\"\"Return the gradient of the model as a flattened vector.\"\"\"\n",
    "    vec = []\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            vec.append(param.grad.detach().reshape(-1))\n",
    "    return torch.cat(vec, 0)\n",
    "\n",
    "def update_grad(model, grad_vec):\n",
    "    \"\"\"Update the model gradients with a new flattened gradient vector.\"\"\"\n",
    "    idx = 0\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            arr_shape = param.grad.shape\n",
    "            size = param.grad.numel()\n",
    "            param.grad.data = grad_vec[idx:idx + size].reshape(arr_shape)\n",
    "            idx += size\n",
    "def load_saved_parameters(save_dir, start_epoch, end_epoch):\n",
    "    W = []\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        param_filename = os.path.join(save_dir, f'{epoch + 1}.pt')\n",
    "        if os.path.exists(param_filename):\n",
    "            model.load_state_dict(torch.load(param_filename))\n",
    "            W.append(get_model_param_vec(model))\n",
    "        else:\n",
    "            print(f'File not found: {param_filename}')\n",
    "    W = np.array(W)\n",
    "    print(f'Loaded {len(W)} parameter vectors with shape: {W.shape}')\n",
    "    return W\n",
    "\n",
    "\n",
    "def get_model_param_vec(model):\n",
    "    vec = []\n",
    "    for name, param in model.named_parameters():\n",
    "        vec.append(param.detach().cpu().numpy().reshape(-1))\n",
    "    return np.concatenate(vec, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6189/3889215893.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(param_filename))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 40 parameter vectors with shape: (40, 11175370)\n",
      "ratio: [0.810835   0.07315665 0.03047929 0.01720474 0.01142453 0.00814474\n",
      " 0.00605126 0.00476951 0.00374862 0.00328506 0.00290904 0.00254596\n",
      " 0.0022525  0.00207408 0.00190761 0.0017146  0.00157571 0.00143144\n",
      " 0.00136058 0.0011949 ]\n",
      "P: (20, 11175370)\n",
      "float32\n",
      "Epoch 1/20 | Train Acc: 99.98% | Test Acc: 99.43%\n",
      "Epoch 2/20 | Train Acc: 99.99% | Test Acc: 99.49%\n",
      "Epoch 3/20 | Train Acc: 99.99% | Test Acc: 99.44%\n",
      "Epoch 4/20 | Train Acc: 99.99% | Test Acc: 99.47%\n",
      "Epoch 5/20 | Train Acc: 99.99% | Test Acc: 99.49%\n",
      "Epoch 6/20 | Train Acc: 99.99% | Test Acc: 99.47%\n",
      "Epoch 7/20 | Train Acc: 99.99% | Test Acc: 99.50%\n",
      "Epoch 8/20 | Train Acc: 99.99% | Test Acc: 99.49%\n",
      "Epoch 9/20 | Train Acc: 100.00% | Test Acc: 99.52%\n",
      "Epoch 10/20 | Train Acc: 100.00% | Test Acc: 99.52%\n",
      "Epoch 11/20 | Train Acc: 99.99% | Test Acc: 99.49%\n",
      "Epoch 12/20 | Train Acc: 100.00% | Test Acc: 99.49%\n",
      "Epoch 13/20 | Train Acc: 100.00% | Test Acc: 99.52%\n",
      "Epoch 14/20 | Train Acc: 100.00% | Test Acc: 99.51%\n",
      "Epoch 15/20 | Train Acc: 100.00% | Test Acc: 99.53%\n",
      "Epoch 16/20 | Train Acc: 100.00% | Test Acc: 99.49%\n",
      "Epoch 17/20 | Train Acc: 100.00% | Test Acc: 99.52%\n",
      "Epoch 18/20 | Train Acc: 100.00% | Test Acc: 99.49%\n",
      "Epoch 19/20 | Train Acc: 100.00% | Test Acc: 99.50%\n",
      "Epoch 20/20 | Train Acc: 100.00% | Test Acc: 99.54%\n"
     ]
    }
   ],
   "source": [
    "#PSGD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "W = load_saved_parameters(save_dir, START_EPOCH, END_EPOCH)\n",
    "  # Obtain base variables through PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit_transform(W)\n",
    "P = np.array(pca.components_)\n",
    "print ('ratio:', pca.explained_variance_ratio_)\n",
    "print ('P:', P.shape)\n",
    "print(P.dtype)\n",
    "\n",
    "P = torch.from_numpy(P).cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "# Training parameters\n",
    "\n",
    "alpha = 0.1  # Learning rate for residual gradient\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_FINE_TUNE):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, probas = model(features)\n",
    "        cost = criterion(logits, targets)\n",
    "\n",
    "        # Backward pass to compute gradients\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "\n",
    "        # Get the full gradient as a vector\n",
    "        grad_vec = []\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_vec.append(param.grad.detach().reshape(-1))\n",
    "        grad_vec = torch.cat(grad_vec, 0)\n",
    "\n",
    "        # Project gradient to the reduced space\n",
    "        gk = torch.mm(P, grad_vec.reshape(-1, 1))\n",
    "        grad_proj = torch.mm(P.T, gk).reshape(-1)\n",
    "\n",
    "        # Compute residual gradient\n",
    "        grad_res = grad_vec - grad_proj\n",
    "\n",
    "        # Update the model parameters using projected gradient\n",
    "        idx = 0\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                size = param.grad.numel()\n",
    "                param.grad.data = grad_proj[idx:idx + size].reshape(param.grad.shape)\n",
    "                idx += size\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update model with residual gradient using a smaller learning rate (alpha)\n",
    "        idx = 0\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                size = param.grad.numel()\n",
    "                param.grad.data = alpha * grad_res[idx:idx + size].reshape(param.grad.shape)\n",
    "                idx += size\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging every 50 batches\n",
    "        #if batch_idx % 50 == 0:\n",
    "        #    print(f'Epoch: {epoch + 1}/{NUM_EPOCHS_FINE_TUNE} | Batch {batch_idx}/{len(train_loader)} | Cost: {cost:.4f}')\n",
    "\n",
    "    # Evaluate the model after each epoch\n",
    "    model.eval()\n",
    "    train_acc = compute_accuracy(model, train_loader, DEVICE)\n",
    "    test_acc = compute_accuracy(model, test_loader, DEVICE)\n",
    "    print(f'Epoch {epoch + 1}/{NUM_EPOCHS_FINE_TUNE} | Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}%')\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6189/3889215893.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(param_filename))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 40 parameter vectors with shape: (40, 11175370)\n",
      "U: (20, 40)\n",
      "P: (11175370, 20)\n",
      "Epoch 1/20 | Train Acc: 99.98% | Test Acc: 99.46%\n",
      "Epoch 2/20 | Train Acc: 99.98% | Test Acc: 99.43%\n",
      "Epoch 3/20 | Train Acc: 99.99% | Test Acc: 99.47%\n",
      "Epoch 4/20 | Train Acc: 99.99% | Test Acc: 99.47%\n",
      "Epoch 5/20 | Train Acc: 99.99% | Test Acc: 99.46%\n",
      "Epoch 6/20 | Train Acc: 99.99% | Test Acc: 99.52%\n",
      "Epoch 7/20 | Train Acc: 99.99% | Test Acc: 99.51%\n",
      "Epoch 8/20 | Train Acc: 99.99% | Test Acc: 99.50%\n",
      "Epoch 9/20 | Train Acc: 99.99% | Test Acc: 99.47%\n",
      "Epoch 10/20 | Train Acc: 100.00% | Test Acc: 99.53%\n",
      "Epoch 11/20 | Train Acc: 100.00% | Test Acc: 99.48%\n",
      "Epoch 12/20 | Train Acc: 100.00% | Test Acc: 99.54%\n",
      "Epoch 13/20 | Train Acc: 100.00% | Test Acc: 99.50%\n",
      "Epoch 14/20 | Train Acc: 100.00% | Test Acc: 99.47%\n",
      "Epoch 15/20 | Train Acc: 100.00% | Test Acc: 99.52%\n",
      "Epoch 16/20 | Train Acc: 100.00% | Test Acc: 99.53%\n",
      "Epoch 17/20 | Train Acc: 100.00% | Test Acc: 99.50%\n",
      "Epoch 18/20 | Train Acc: 100.00% | Test Acc: 99.47%\n",
      "Epoch 19/20 | Train Acc: 100.00% | Test Acc: 99.49%\n",
      "Epoch 20/20 | Train Acc: 100.00% | Test Acc: 99.52%\n"
     ]
    }
   ],
   "source": [
    "#TME\n",
    "\n",
    "\n",
    "def m_estimator(X):\n",
    "    N, D = X.shape\n",
    "    initcov = np.eye(D)  \n",
    "    oldcov = initcov - 1\n",
    "    cov = initcov\n",
    "    iter_count = 1\n",
    "    eps = 1e-10  \n",
    "\n",
    "    while np.linalg.norm(oldcov - cov, 'fro') > 1e-12 and iter_count < 1000:\n",
    "        temp = X @ np.linalg.inv(cov + eps * np.eye(D))  \n",
    "        d = np.sum(temp * np.conjugate(X), axis=1)  \n",
    "        oldcov = cov\n",
    "\n",
    "       \n",
    "        temp = (np.real(d) + eps * np.ones(N))**(-1)  \n",
    "\n",
    "        \n",
    "        temp_matrix = np.diag(temp)  \n",
    "        cov = (X.T @ temp_matrix @ X) / (N * D)  \n",
    "        cov = cov / np.trace(cov)  \n",
    "        iter_count += 1  \n",
    "\n",
    "    return cov\n",
    "\n",
    "\n",
    "\n",
    "W = load_saved_parameters(save_dir, START_EPOCH, END_EPOCH)\n",
    "  # Obtain base variables through PCA\n",
    "V = np.dot(W,W.T)\n",
    "W_hat = sqrtm(V)\n",
    "   \n",
    "Cov = m_estimator(W_hat)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit_transform(Cov)\n",
    "U = np.array(pca.components_)\n",
    "print('U:',U.shape)\n",
    "P =  (W.T) @ LA.inv(W_hat) @ (U.T)\n",
    "    \n",
    "print ('P:', P.shape)\n",
    "P = P.T\n",
    "P = P.astype(np.float32)\n",
    "\n",
    "P = torch.from_numpy(P).cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "# Training parameters\n",
    "\n",
    "alpha = 0.1  # Learning rate for residual gradient\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_FINE_TUNE):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, probas = model(features)\n",
    "        cost = criterion(logits, targets)\n",
    "\n",
    "        # Backward pass to compute gradients\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "\n",
    "        # Get the full gradient as a vector\n",
    "        grad_vec = []\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_vec.append(param.grad.detach().reshape(-1))\n",
    "        grad_vec = torch.cat(grad_vec, 0)\n",
    "\n",
    "        # Project gradient to the reduced space\n",
    "        gk = torch.mm(P, grad_vec.reshape(-1, 1))\n",
    "        grad_proj = torch.mm(P.T, gk).reshape(-1)\n",
    "\n",
    "        # Compute residual gradient\n",
    "        grad_res = grad_vec - grad_proj\n",
    "\n",
    "        # Update the model parameters using projected gradient\n",
    "        idx = 0\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                size = param.grad.numel()\n",
    "                param.grad.data = grad_proj[idx:idx + size].reshape(param.grad.shape)\n",
    "                idx += size\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update model with residual gradient using a smaller learning rate (alpha)\n",
    "        idx = 0\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                size = param.grad.numel()\n",
    "                param.grad.data = alpha * grad_res[idx:idx + size].reshape(param.grad.shape)\n",
    "                idx += size\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging every 50 batches\n",
    "        #if batch_idx % 50 == 0:\n",
    "        #    print(f'Epoch: {epoch + 1}/{NUM_EPOCHS_FINE_TUNE} | Batch {batch_idx}/{len(train_loader)} | Cost: {cost:.4f}')\n",
    "\n",
    "    # Evaluate the model after each epoch\n",
    "    model.eval()\n",
    "    train_acc = compute_accuracy(model, train_loader, DEVICE)\n",
    "    test_acc = compute_accuracy(model, test_loader, DEVICE)\n",
    "    print(f'Epoch {epoch + 1}/{NUM_EPOCHS_FINE_TUNE} | Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}%')\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6189/3889215893.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(param_filename))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 40 parameter vectors with shape: (40, 11175370)\n",
      "P: (11175370, 20)\n",
      "Epoch 1/20 | Train Acc: 99.98% | Test Acc: 99.44%\n",
      "Epoch 2/20 | Train Acc: 99.98% | Test Acc: 99.46%\n",
      "Epoch 3/20 | Train Acc: 99.99% | Test Acc: 99.51%\n",
      "Epoch 4/20 | Train Acc: 99.99% | Test Acc: 99.44%\n",
      "Epoch 5/20 | Train Acc: 99.99% | Test Acc: 99.49%\n",
      "Epoch 6/20 | Train Acc: 99.99% | Test Acc: 99.45%\n",
      "Epoch 7/20 | Train Acc: 99.99% | Test Acc: 99.47%\n",
      "Epoch 8/20 | Train Acc: 99.99% | Test Acc: 99.47%\n",
      "Epoch 9/20 | Train Acc: 100.00% | Test Acc: 99.48%\n",
      "Epoch 10/20 | Train Acc: 100.00% | Test Acc: 99.45%\n",
      "Epoch 11/20 | Train Acc: 100.00% | Test Acc: 99.51%\n",
      "Epoch 12/20 | Train Acc: 100.00% | Test Acc: 99.48%\n",
      "Epoch 13/20 | Train Acc: 100.00% | Test Acc: 99.47%\n",
      "Epoch 14/20 | Train Acc: 100.00% | Test Acc: 99.52%\n",
      "Epoch 15/20 | Train Acc: 100.00% | Test Acc: 99.53%\n",
      "Epoch 16/20 | Train Acc: 100.00% | Test Acc: 99.50%\n",
      "Epoch 17/20 | Train Acc: 100.00% | Test Acc: 99.50%\n",
      "Epoch 18/20 | Train Acc: 100.00% | Test Acc: 99.49%\n",
      "Epoch 19/20 | Train Acc: 100.00% | Test Acc: 99.50%\n",
      "Epoch 20/20 | Train Acc: 100.00% | Test Acc: 99.51%\n"
     ]
    }
   ],
   "source": [
    "#FMS\n",
    "\n",
    "def FMS(X, dd):\n",
    "    D, N = X.shape\n",
    "\n",
    "    # Initial iteration count\n",
    "    iter = 1\n",
    "\n",
    "    # Perform SVD and initialize L\n",
    "    U, _, _ = svd(X, full_matrices=False)\n",
    "    L = U[:, :dd]\n",
    "\n",
    "    # Set initial angle and tolerance\n",
    "    ang = 1\n",
    "\n",
    "    # Iterate until convergence or max iteration count\n",
    "    while ang > 1e-12 and iter < 1000:\n",
    "        Lold = L\n",
    "\n",
    "        # Compute the residual projection\n",
    "        temp = (np.eye(D) - L @ L.T) @ X\n",
    "        w = np.sqrt(np.sum(temp**2, axis=0)) + 1e-10\n",
    "\n",
    "        # Reweight and update XX\n",
    "        XX = X @ np.diag(1.0 / w) @ X.T\n",
    "\n",
    "        # Perform SVD again on the weighted matrix XX\n",
    "        U, _, _ = svd(XX, full_matrices=False)\n",
    "        L = U[:, :dd]\n",
    "\n",
    "        # Compute the angle between new and old subspace\n",
    "        ang = np.linalg.norm(subspace_angles(L, Lold))\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "    return L\n",
    "\n",
    "W = load_saved_parameters(save_dir, START_EPOCH, END_EPOCH)\n",
    "  # Obtain base variables through PCA\n",
    "V = np.dot(W,W.T)\n",
    "W_hat = sqrtm(V)\n",
    "n_components = n_components\n",
    "U= FMS(W_hat,n_components)\n",
    "    \n",
    "P =  (W.T) @ LA.inv(W_hat) @ (U)\n",
    "    \n",
    "print ('P:', P.shape)\n",
    "P = P.T\n",
    "P = P.astype(np.float32)\n",
    "P = torch.from_numpy(P).cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "# Training parameters\n",
    "\n",
    "alpha = 0.1  # Learning rate for residual gradient\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_FINE_TUNE):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, probas = model(features)\n",
    "        cost = criterion(logits, targets)\n",
    "\n",
    "        # Backward pass to compute gradients\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "\n",
    "        # Get the full gradient as a vector\n",
    "        grad_vec = []\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_vec.append(param.grad.detach().reshape(-1))\n",
    "        grad_vec = torch.cat(grad_vec, 0)\n",
    "\n",
    "        # Project gradient to the reduced space\n",
    "        gk = torch.mm(P, grad_vec.reshape(-1, 1))\n",
    "        grad_proj = torch.mm(P.T, gk).reshape(-1)\n",
    "\n",
    "        # Compute residual gradient\n",
    "        grad_res = grad_vec - grad_proj\n",
    "\n",
    "        # Update the model parameters using projected gradient\n",
    "        idx = 0\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                size = param.grad.numel()\n",
    "                param.grad.data = grad_proj[idx:idx + size].reshape(param.grad.shape)\n",
    "                idx += size\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update model with residual gradient using a smaller learning rate (alpha)\n",
    "        idx = 0\n",
    "        for param in model.parameters():\n",
    "            if param.grad is not None:\n",
    "                size = param.grad.numel()\n",
    "                param.grad.data = alpha * grad_res[idx:idx + size].reshape(param.grad.shape)\n",
    "                idx += size\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging every 50 batches\n",
    "        #if batch_idx % 50 == 0:\n",
    "        #    print(f'Epoch: {epoch + 1}/{NUM_EPOCHS_FINE_TUNE} | Batch {batch_idx}/{len(train_loader)} | Cost: {cost:.4f}')\n",
    "\n",
    "    # Evaluate the model after each epoch\n",
    "    model.eval()\n",
    "    train_acc = compute_accuracy(model, train_loader, DEVICE)\n",
    "    test_acc = compute_accuracy(model, test_loader, DEVICE)\n",
    "    print(f'Epoch {epoch + 1}/{NUM_EPOCHS_FINE_TUNE} | Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}%')\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
